{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def filter_by_knowledge(int_grad: Dict[str, Dict[str, Any]], samples_to_eval: Dict[str, Dict[str, Any]], with_knowledge: bool):\n",
    "    subset = {}\n",
    "    for sample_id, sample in int_grad.items():\n",
    "        if samples_to_eval[sample_id][\"has_knowledge\"] == with_knowledge:\n",
    "            subset[sample_id] = sample\n",
    "    return subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WoW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simone/miniconda3/envs/inlg/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from utils import compute_average_attribution\n",
    "\n",
    "# top_k_percentage tokens with highest attribution\n",
    "top_k_percentage = 0.25\n",
    "\n",
    "scores = {}\n",
    "top_k_scores = {}\n",
    "for model in Path(\"../output/WizardOfWikipedia/\").iterdir():\n",
    "    if model.is_dir():\n",
    "        if model.name == \"mistral\":\n",
    "            # name of the segments\n",
    "            segments = [\"topic\", \"dialogue_history\", \"knowledge\"]\n",
    "            # tokens to separate the segments\n",
    "            tokens_to_find = [[7082, 441, 28747], [11308, 3829, 28747]]\n",
    "            # tokens to remove from the attribution\n",
    "            tokens_to_remove = [1, 2, 28705, 733, 16289, 28793, 28748]\n",
    "            for item in model.iterdir():\n",
    "                if item.is_dir():\n",
    "                    for file in item.iterdir():\n",
    "                        if file.name.startswith(\"integrated_gradients\"):\n",
    "                            with open(file, \"rb\") as f:\n",
    "                                int_grad = pickle.load(f)\n",
    "                            sc, top_k_sc = compute_average_attribution(int_grad, top_k_percentage, tokens_to_remove, tokens_to_find)    \n",
    "                            scores[f'{model.name}_ft_{file.name.split(\"integrated_gradients_\").pop()[:-4]}'] = {k:v for k, v in zip(segments, sc)}\n",
    "                            top_k_scores[f'{model.name}_ft_{file.name.split(\"integrated_gradients_\").pop()[:-4]}'] = {k:v for k, v in zip(segments, top_k_sc)}\n",
    "                elif item.name.startswith(\"integrated_gradients\"):\n",
    "                    with open(item, \"rb\") as f:\n",
    "                        int_grad = pickle.load(f)\n",
    "                    sc, top_k_sc = compute_average_attribution(int_grad, top_k_percentage, tokens_to_remove, tokens_to_find)\n",
    "                    scores[f'{model.name}_prompt_{item.name.split(\"integrated_gradients_\").pop()[:-4]}'] = {k:v for k, v in zip(segments, sc)}\n",
    "                    top_k_scores[f'{model.name}_prompt_{item.name.split(\"integrated_gradients_\").pop()[:-4]}'] = {k:v for k, v in zip(segments, top_k_sc)}\n",
    "        else:\n",
    "            # tokens to remove from the attribution\n",
    "            tokens_to_remove = [1, 2, 518, 25580, 29962, 3532, 14816, 29903, 6778, 29966, 829]\n",
    "            for item in model.iterdir():\n",
    "                if item.is_dir():\n",
    "                    # name of the segments\n",
    "                    segments = [\"topic\", \"dialogue_history\", \"knowledge\"]\n",
    "                    # tokens to separate the segments\n",
    "                    tokens_to_find = [[7647, 434, 29901], [19320, 5485, 29901]]\n",
    "                    for file in item.iterdir():\n",
    "                        if file.name.startswith(\"integrated_gradients\"):\n",
    "                            with open(file, \"rb\") as f:\n",
    "                                int_grad = pickle.load(f)\n",
    "                            sc, top_k_sc = compute_average_attribution(int_grad, top_k_percentage, tokens_to_remove, tokens_to_find)    \n",
    "                            scores[f'{model.name}_ft_{file.name.split(\"integrated_gradients_\").pop()[:-4]}'] = {k:v for k, v in zip(segments, sc)}\n",
    "                            top_k_scores[f'{model.name}_ft_{file.name.split(\"integrated_gradients_\").pop()[:-4]}'] = {k:v for k, v in zip(segments, top_k_sc)}\n",
    "                elif item.name.startswith(\"integrated_gradients\"):\n",
    "                    # name of the segments\n",
    "                    segments = [\"prompt\", \"topic\", \"dialogue_history\", \"knowledge\"]\n",
    "                    # tokens to separate the segments\n",
    "                    tokens_to_find = [[13, 7031, 293, 29901], [7647, 434, 29901], [19320, 5485, 29901]]\n",
    "                    with open(item, \"rb\") as f:\n",
    "                        int_grad = pickle.load(f)\n",
    "                    sc, top_k_sc = compute_average_attribution(int_grad, top_k_percentage, tokens_to_remove, tokens_to_find)\n",
    "                    scores[f'{model.name}_prompt_{item.name.split(\"integrated_gradients_\").pop()[:-4]}'] = {k:v for k, v in zip(segments, sc)}\n",
    "                    top_k_scores[f'{model.name}_prompt_{item.name.split(\"integrated_gradients_\").pop()[:-4]}'] = {k:v for k, v in zip(segments, top_k_sc)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>topic</th>\n",
       "      <th>dialogue_history</th>\n",
       "      <th>knowledge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>llama_ft_gold</th>\n",
       "      <td>NaN</td>\n",
       "      <td>49.22</td>\n",
       "      <td>16.35</td>\n",
       "      <td>34.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama_ft_retrieved_top-3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>51.58</td>\n",
       "      <td>22.90</td>\n",
       "      <td>25.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama_prompt_retrieved_top-1</th>\n",
       "      <td>21.42</td>\n",
       "      <td>28.90</td>\n",
       "      <td>17.49</td>\n",
       "      <td>32.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama_ft_none</th>\n",
       "      <td>NaN</td>\n",
       "      <td>60.99</td>\n",
       "      <td>19.81</td>\n",
       "      <td>19.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama_prompt_none</th>\n",
       "      <td>24.47</td>\n",
       "      <td>30.68</td>\n",
       "      <td>17.18</td>\n",
       "      <td>27.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama_ft_retrieved_top-1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>50.37</td>\n",
       "      <td>16.55</td>\n",
       "      <td>33.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama_prompt_gold</th>\n",
       "      <td>21.15</td>\n",
       "      <td>27.96</td>\n",
       "      <td>15.61</td>\n",
       "      <td>35.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama_prompt_retrieved_top-3</th>\n",
       "      <td>25.72</td>\n",
       "      <td>28.92</td>\n",
       "      <td>15.81</td>\n",
       "      <td>29.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral_ft_gold</th>\n",
       "      <td>NaN</td>\n",
       "      <td>69.60</td>\n",
       "      <td>13.62</td>\n",
       "      <td>16.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral_prompt_retrieved_top-1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>66.09</td>\n",
       "      <td>15.08</td>\n",
       "      <td>18.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral_ft_retrieved_top-3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>66.72</td>\n",
       "      <td>19.25</td>\n",
       "      <td>14.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral_ft_none</th>\n",
       "      <td>NaN</td>\n",
       "      <td>76.88</td>\n",
       "      <td>16.83</td>\n",
       "      <td>6.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral_prompt_none</th>\n",
       "      <td>NaN</td>\n",
       "      <td>67.74</td>\n",
       "      <td>14.07</td>\n",
       "      <td>18.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral_ft_retrieved_top-1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>68.64</td>\n",
       "      <td>16.66</td>\n",
       "      <td>14.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral_prompt_gold</th>\n",
       "      <td>NaN</td>\n",
       "      <td>65.66</td>\n",
       "      <td>13.88</td>\n",
       "      <td>20.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral_prompt_retrieved_top-3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>63.85</td>\n",
       "      <td>18.69</td>\n",
       "      <td>17.46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                prompt  topic  dialogue_history  knowledge\n",
       "llama_ft_gold                      NaN  49.22             16.35      34.43\n",
       "llama_ft_retrieved_top-3           NaN  51.58             22.90      25.52\n",
       "llama_prompt_retrieved_top-1     21.42  28.90             17.49      32.19\n",
       "llama_ft_none                      NaN  60.99             19.81      19.20\n",
       "llama_prompt_none                24.47  30.68             17.18      27.67\n",
       "llama_ft_retrieved_top-1           NaN  50.37             16.55      33.08\n",
       "llama_prompt_gold                21.15  27.96             15.61      35.28\n",
       "llama_prompt_retrieved_top-3     25.72  28.92             15.81      29.55\n",
       "mistral_ft_gold                    NaN  69.60             13.62      16.78\n",
       "mistral_prompt_retrieved_top-1     NaN  66.09             15.08      18.82\n",
       "mistral_ft_retrieved_top-3         NaN  66.72             19.25      14.03\n",
       "mistral_ft_none                    NaN  76.88             16.83       6.29\n",
       "mistral_prompt_none                NaN  67.74             14.07      18.19\n",
       "mistral_ft_retrieved_top-1         NaN  68.64             16.66      14.70\n",
       "mistral_prompt_gold                NaN  65.66             13.88      20.45\n",
       "mistral_prompt_retrieved_top-3     NaN  63.85             18.69      17.46"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(top_k_scores).T * 100\n",
    "display(df[[\"prompt\", \"topic\", \"dialogue_history\", \"knowledge\"]].round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knowledge only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import compute_average_attribution\n",
    "\n",
    "with open(\"../data/WizardOfWikipedia/samples_to_eval.json\", \"r\") as f:\n",
    "    samples_to_eval = json.load(f)\n",
    "\n",
    "# top_k_percentage tokens with highest attribution\n",
    "top_k_percentage = 0.25\n",
    "\n",
    "with_knowledge = True\n",
    "\n",
    "scores = {}\n",
    "top_k_scores = {}\n",
    "for model in Path(\"../output/WizardOfWikipedia/\").iterdir():\n",
    "    if model.is_dir():\n",
    "        if model.name == \"mistral\":\n",
    "            # name of the segments\n",
    "            segments = [\"topic\", \"dialogue_history\", \"knowledge\"]\n",
    "            # tokens to separate the segments\n",
    "            tokens_to_find = [[7082, 441, 28747], [11308, 3829, 28747]]\n",
    "            # tokens to remove from the attribution\n",
    "            tokens_to_remove = [1, 2, 28705, 733, 16289, 28793, 28748]\n",
    "            for item in model.iterdir():\n",
    "                if item.is_dir():\n",
    "                    for file in item.iterdir():\n",
    "                        if file.name.startswith(\"integrated_gradients\"):\n",
    "                            with open(file, \"rb\") as f:\n",
    "                                int_grad = pickle.load(f)\n",
    "                                int_grad = filter_by_knowledge(int_grad, samples_to_eval, with_knowledge)\n",
    "                            sc, top_k_sc = compute_average_attribution(int_grad, top_k_percentage, tokens_to_remove, tokens_to_find)    \n",
    "                            scores[f'{model.name}_ft_{file.name.split(\"integrated_gradients_\").pop()[:-4]}'] = {k:v for k, v in zip(segments, sc)}\n",
    "                            top_k_scores[f'{model.name}_ft_{file.name.split(\"integrated_gradients_\").pop()[:-4]}'] = {k:v for k, v in zip(segments, top_k_sc)}\n",
    "                elif item.name.startswith(\"integrated_gradients\"):\n",
    "                    with open(item, \"rb\") as f:\n",
    "                        int_grad = pickle.load(f)\n",
    "                        int_grad = filter_by_knowledge(int_grad, samples_to_eval, with_knowledge)\n",
    "                    sc, top_k_sc = compute_average_attribution(int_grad, top_k_percentage, tokens_to_remove, tokens_to_find)\n",
    "                    scores[f'{model.name}_prompt_{item.name.split(\"integrated_gradients_\").pop()[:-4]}'] = {k:v for k, v in zip(segments, sc)}\n",
    "                    top_k_scores[f'{model.name}_prompt_{item.name.split(\"integrated_gradients_\").pop()[:-4]}'] = {k:v for k, v in zip(segments, top_k_sc)}\n",
    "        else:\n",
    "            # tokens to remove from the attribution\n",
    "            tokens_to_remove = [1, 2, 518, 25580, 29962, 3532, 14816, 29903, 6778, 29966, 829]\n",
    "            for item in model.iterdir():\n",
    "                if item.is_dir():\n",
    "                    # name of the segments\n",
    "                    segments = [\"topic\", \"dialogue_history\", \"knowledge\"]\n",
    "                    # tokens to separate the segments\n",
    "                    tokens_to_find = [[7647, 434, 29901], [19320, 5485, 29901]]\n",
    "                    for file in item.iterdir():\n",
    "                        if file.name.startswith(\"integrated_gradients\"):\n",
    "                            with open(file, \"rb\") as f:\n",
    "                                int_grad = pickle.load(f)\n",
    "                                int_grad = filter_by_knowledge(int_grad, samples_to_eval, with_knowledge)\n",
    "                            sc, top_k_sc = compute_average_attribution(int_grad, top_k_percentage, tokens_to_remove, tokens_to_find)    \n",
    "                            scores[f'{model.name}_ft_{file.name.split(\"integrated_gradients_\").pop()[:-4]}'] = {k:v for k, v in zip(segments, sc)}\n",
    "                            top_k_scores[f'{model.name}_ft_{file.name.split(\"integrated_gradients_\").pop()[:-4]}'] = {k:v for k, v in zip(segments, top_k_sc)}\n",
    "                elif item.name.startswith(\"integrated_gradients\"):\n",
    "                    # name of the segments\n",
    "                    segments = [\"prompt\", \"topic\", \"dialogue_history\", \"knowledge\"]\n",
    "                    # tokens to separate the segments\n",
    "                    tokens_to_find = [[13, 7031, 293, 29901], [7647, 434, 29901], [19320, 5485, 29901]]\n",
    "                    with open(item, \"rb\") as f:\n",
    "                        int_grad = pickle.load(f)\n",
    "                        int_grad = filter_by_knowledge(int_grad, samples_to_eval, with_knowledge)\n",
    "                    sc, top_k_sc = compute_average_attribution(int_grad, top_k_percentage, tokens_to_remove, tokens_to_find)\n",
    "                    scores[f'{model.name}_prompt_{item.name.split(\"integrated_gradients_\").pop()[:-4]}'] = {k:v for k, v in zip(segments, sc)}\n",
    "                    top_k_scores[f'{model.name}_prompt_{item.name.split(\"integrated_gradients_\").pop()[:-4]}'] = {k:v for k, v in zip(segments, top_k_sc)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>topic</th>\n",
       "      <th>dialogue_history</th>\n",
       "      <th>knowledge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>llama_ft_gold</th>\n",
       "      <td>NaN</td>\n",
       "      <td>39.43</td>\n",
       "      <td>13.80</td>\n",
       "      <td>46.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama_ft_retrieved_top-3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>51.68</td>\n",
       "      <td>22.71</td>\n",
       "      <td>25.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama_prompt_retrieved_top-1</th>\n",
       "      <td>24.24</td>\n",
       "      <td>30.54</td>\n",
       "      <td>18.53</td>\n",
       "      <td>26.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama_ft_none</th>\n",
       "      <td>NaN</td>\n",
       "      <td>60.83</td>\n",
       "      <td>18.64</td>\n",
       "      <td>20.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama_prompt_none</th>\n",
       "      <td>27.22</td>\n",
       "      <td>33.44</td>\n",
       "      <td>17.99</td>\n",
       "      <td>21.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama_ft_retrieved_top-1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>60.01</td>\n",
       "      <td>19.96</td>\n",
       "      <td>20.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama_prompt_gold</th>\n",
       "      <td>21.85</td>\n",
       "      <td>28.60</td>\n",
       "      <td>15.96</td>\n",
       "      <td>33.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama_prompt_retrieved_top-3</th>\n",
       "      <td>29.14</td>\n",
       "      <td>30.34</td>\n",
       "      <td>15.54</td>\n",
       "      <td>24.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral_ft_gold</th>\n",
       "      <td>NaN</td>\n",
       "      <td>65.55</td>\n",
       "      <td>11.00</td>\n",
       "      <td>23.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral_prompt_retrieved_top-1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>70.68</td>\n",
       "      <td>17.50</td>\n",
       "      <td>11.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral_ft_retrieved_top-3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>67.56</td>\n",
       "      <td>24.59</td>\n",
       "      <td>7.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral_ft_none</th>\n",
       "      <td>NaN</td>\n",
       "      <td>77.43</td>\n",
       "      <td>16.59</td>\n",
       "      <td>5.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral_prompt_none</th>\n",
       "      <td>NaN</td>\n",
       "      <td>72.32</td>\n",
       "      <td>15.01</td>\n",
       "      <td>12.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral_ft_retrieved_top-1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>71.56</td>\n",
       "      <td>19.47</td>\n",
       "      <td>8.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral_prompt_gold</th>\n",
       "      <td>NaN</td>\n",
       "      <td>69.02</td>\n",
       "      <td>14.89</td>\n",
       "      <td>16.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral_prompt_retrieved_top-3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>66.92</td>\n",
       "      <td>25.01</td>\n",
       "      <td>8.07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                prompt  topic  dialogue_history  knowledge\n",
       "llama_ft_gold                      NaN  39.43             13.80      46.78\n",
       "llama_ft_retrieved_top-3           NaN  51.68             22.71      25.61\n",
       "llama_prompt_retrieved_top-1     24.24  30.54             18.53      26.69\n",
       "llama_ft_none                      NaN  60.83             18.64      20.53\n",
       "llama_prompt_none                27.22  33.44             17.99      21.34\n",
       "llama_ft_retrieved_top-1           NaN  60.01             19.96      20.04\n",
       "llama_prompt_gold                21.85  28.60             15.96      33.58\n",
       "llama_prompt_retrieved_top-3     29.14  30.34             15.54      24.98\n",
       "mistral_ft_gold                    NaN  65.55             11.00      23.45\n",
       "mistral_prompt_retrieved_top-1     NaN  70.68             17.50      11.83\n",
       "mistral_ft_retrieved_top-3         NaN  67.56             24.59       7.85\n",
       "mistral_ft_none                    NaN  77.43             16.59       5.98\n",
       "mistral_prompt_none                NaN  72.32             15.01      12.67\n",
       "mistral_ft_retrieved_top-1         NaN  71.56             19.47       8.97\n",
       "mistral_prompt_gold                NaN  69.02             14.89      16.10\n",
       "mistral_prompt_retrieved_top-3     NaN  66.92             25.01       8.07"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(top_k_scores).T * 100\n",
    "display(df[[\"prompt\", \"topic\", \"dialogue_history\", \"knowledge\"]].round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import compute_average_attribution\n",
    "\n",
    "with open(\"../data/WizardOfWikipedia/samples_to_eval.json\", \"r\") as f:\n",
    "    samples_to_eval = json.load(f)\n",
    "\n",
    "# top_k_percentage tokens with highest attribution\n",
    "top_k_percentage = 0.25\n",
    "\n",
    "with_knowledge = False\n",
    "\n",
    "scores = {}\n",
    "top_k_scores = {}\n",
    "for model in Path(\"../output/WizardOfWikipedia/\").iterdir():\n",
    "    if model.is_dir():\n",
    "        if model.name == \"mistral\":\n",
    "            # name of the segments\n",
    "            segments = [\"topic\", \"dialogue_history\", \"knowledge\"]\n",
    "            # tokens to separate the segments\n",
    "            tokens_to_find = [[7082, 441, 28747], [11308, 3829, 28747]]\n",
    "            # tokens to remove from the attribution\n",
    "            tokens_to_remove = [1, 2, 28705, 733, 16289, 28793, 28748]\n",
    "            for item in model.iterdir():\n",
    "                if item.is_dir():\n",
    "                    for file in item.iterdir():\n",
    "                        if file.name.startswith(\"integrated_gradients\"):\n",
    "                            with open(file, \"rb\") as f:\n",
    "                                int_grad = pickle.load(f)\n",
    "                                int_grad = filter_by_knowledge(int_grad, samples_to_eval, with_knowledge)\n",
    "                            sc, top_k_sc = compute_average_attribution(int_grad, top_k_percentage, tokens_to_remove, tokens_to_find)    \n",
    "                            scores[f'{model.name}_ft_{file.name.split(\"integrated_gradients_\").pop()[:-4]}'] = {k:v for k, v in zip(segments, sc)}\n",
    "                            top_k_scores[f'{model.name}_ft_{file.name.split(\"integrated_gradients_\").pop()[:-4]}'] = {k:v for k, v in zip(segments, top_k_sc)}\n",
    "                elif item.name.startswith(\"integrated_gradients\"):\n",
    "                    with open(item, \"rb\") as f:\n",
    "                        int_grad = pickle.load(f)\n",
    "                        int_grad = filter_by_knowledge(int_grad, samples_to_eval, with_knowledge)\n",
    "                    sc, top_k_sc = compute_average_attribution(int_grad, top_k_percentage, tokens_to_remove, tokens_to_find)\n",
    "                    scores[f'{model.name}_prompt_{item.name.split(\"integrated_gradients_\").pop()[:-4]}'] = {k:v for k, v in zip(segments, sc)}\n",
    "                    top_k_scores[f'{model.name}_prompt_{item.name.split(\"integrated_gradients_\").pop()[:-4]}'] = {k:v for k, v in zip(segments, top_k_sc)}\n",
    "        else:\n",
    "            # tokens to remove from the attribution\n",
    "            tokens_to_remove = [1, 2, 518, 25580, 29962, 3532, 14816, 29903, 6778, 29966, 829]\n",
    "            for item in model.iterdir():\n",
    "                if item.is_dir():\n",
    "                    # name of the segments\n",
    "                    segments = [\"topic\", \"dialogue_history\", \"knowledge\"]\n",
    "                    # tokens to separate the segments\n",
    "                    tokens_to_find = [[7647, 434, 29901], [19320, 5485, 29901]]\n",
    "                    for file in item.iterdir():\n",
    "                        if file.name.startswith(\"integrated_gradients\"):\n",
    "                            with open(file, \"rb\") as f:\n",
    "                                int_grad = pickle.load(f)\n",
    "                                int_grad = filter_by_knowledge(int_grad, samples_to_eval, with_knowledge)\n",
    "                            sc, top_k_sc = compute_average_attribution(int_grad, top_k_percentage, tokens_to_remove, tokens_to_find)    \n",
    "                            scores[f'{model.name}_ft_{file.name.split(\"integrated_gradients_\").pop()[:-4]}'] = {k:v for k, v in zip(segments, sc)}\n",
    "                            top_k_scores[f'{model.name}_ft_{file.name.split(\"integrated_gradients_\").pop()[:-4]}'] = {k:v for k, v in zip(segments, top_k_sc)}\n",
    "                elif item.name.startswith(\"integrated_gradients\"):\n",
    "                    # name of the segments\n",
    "                    segments = [\"prompt\", \"topic\", \"dialogue_history\", \"knowledge\"]\n",
    "                    # tokens to separate the segments\n",
    "                    tokens_to_find = [[13, 7031, 293, 29901], [7647, 434, 29901], [19320, 5485, 29901]]\n",
    "                    with open(item, \"rb\") as f:\n",
    "                        int_grad = pickle.load(f)\n",
    "                        int_grad = filter_by_knowledge(int_grad, samples_to_eval, with_knowledge)\n",
    "                    sc, top_k_sc = compute_average_attribution(int_grad, top_k_percentage, tokens_to_remove, tokens_to_find)\n",
    "                    scores[f'{model.name}_prompt_{item.name.split(\"integrated_gradients_\").pop()[:-4]}'] = {k:v for k, v in zip(segments, sc)}\n",
    "                    top_k_scores[f'{model.name}_prompt_{item.name.split(\"integrated_gradients_\").pop()[:-4]}'] = {k:v for k, v in zip(segments, top_k_sc)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>topic</th>\n",
       "      <th>dialogue_history</th>\n",
       "      <th>knowledge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>llama_ft_gold</th>\n",
       "      <td>NaN</td>\n",
       "      <td>59.01</td>\n",
       "      <td>18.90</td>\n",
       "      <td>22.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama_ft_retrieved_top-3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>51.48</td>\n",
       "      <td>23.10</td>\n",
       "      <td>25.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama_prompt_retrieved_top-1</th>\n",
       "      <td>18.60</td>\n",
       "      <td>27.26</td>\n",
       "      <td>16.45</td>\n",
       "      <td>37.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama_ft_none</th>\n",
       "      <td>NaN</td>\n",
       "      <td>61.15</td>\n",
       "      <td>20.98</td>\n",
       "      <td>17.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama_prompt_none</th>\n",
       "      <td>21.71</td>\n",
       "      <td>27.92</td>\n",
       "      <td>16.37</td>\n",
       "      <td>34.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama_ft_retrieved_top-1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>40.73</td>\n",
       "      <td>13.14</td>\n",
       "      <td>46.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama_prompt_gold</th>\n",
       "      <td>20.44</td>\n",
       "      <td>27.32</td>\n",
       "      <td>15.26</td>\n",
       "      <td>36.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama_prompt_retrieved_top-3</th>\n",
       "      <td>22.31</td>\n",
       "      <td>27.50</td>\n",
       "      <td>16.08</td>\n",
       "      <td>34.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral_ft_gold</th>\n",
       "      <td>NaN</td>\n",
       "      <td>73.66</td>\n",
       "      <td>16.24</td>\n",
       "      <td>10.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral_prompt_retrieved_top-1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>61.51</td>\n",
       "      <td>12.67</td>\n",
       "      <td>25.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral_ft_retrieved_top-3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>65.87</td>\n",
       "      <td>13.92</td>\n",
       "      <td>20.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral_ft_none</th>\n",
       "      <td>NaN</td>\n",
       "      <td>76.33</td>\n",
       "      <td>17.07</td>\n",
       "      <td>6.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral_prompt_none</th>\n",
       "      <td>NaN</td>\n",
       "      <td>63.16</td>\n",
       "      <td>13.13</td>\n",
       "      <td>23.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral_ft_retrieved_top-1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>65.72</td>\n",
       "      <td>13.84</td>\n",
       "      <td>20.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral_prompt_gold</th>\n",
       "      <td>NaN</td>\n",
       "      <td>62.31</td>\n",
       "      <td>12.88</td>\n",
       "      <td>24.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral_prompt_retrieved_top-3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>60.77</td>\n",
       "      <td>12.37</td>\n",
       "      <td>26.86</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                prompt  topic  dialogue_history  knowledge\n",
       "llama_ft_gold                      NaN  59.01             18.90      22.09\n",
       "llama_ft_retrieved_top-3           NaN  51.48             23.10      25.42\n",
       "llama_prompt_retrieved_top-1     18.60  27.26             16.45      37.69\n",
       "llama_ft_none                      NaN  61.15             20.98      17.87\n",
       "llama_prompt_none                21.71  27.92             16.37      34.00\n",
       "llama_ft_retrieved_top-1           NaN  40.73             13.14      46.13\n",
       "llama_prompt_gold                20.44  27.32             15.26      36.99\n",
       "llama_prompt_retrieved_top-3     22.31  27.50             16.08      34.11\n",
       "mistral_ft_gold                    NaN  73.66             16.24      10.10\n",
       "mistral_prompt_retrieved_top-1     NaN  61.51             12.67      25.81\n",
       "mistral_ft_retrieved_top-3         NaN  65.87             13.92      20.21\n",
       "mistral_ft_none                    NaN  76.33             17.07       6.60\n",
       "mistral_prompt_none                NaN  63.16             13.13      23.71\n",
       "mistral_ft_retrieved_top-1         NaN  65.72             13.84      20.43\n",
       "mistral_prompt_gold                NaN  62.31             12.88      24.81\n",
       "mistral_prompt_retrieved_top-3     NaN  60.77             12.37      26.86"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(top_k_scores).T * 100\n",
    "display(df[[\"prompt\", \"topic\", \"dialogue_history\", \"knowledge\"]].round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DSTC9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import compute_average_attribution\n",
    "\n",
    "# top_k_percentage tokens with highest attribution\n",
    "top_k_percentage = 0.25\n",
    "\n",
    "scores = {}\n",
    "top_k_scores = {}\n",
    "for model in Path(\"../output/DSTC9/\").iterdir():\n",
    "    if model.is_dir():\n",
    "        if model.name == \"mistral\":\n",
    "            # tokens to remove from the attribution\n",
    "            tokens_to_remove = [1, 2, 28705, 733, 16289, 28793, 28748]\n",
    "            for item in model.iterdir():\n",
    "                if item.is_dir():\n",
    "                    # name of the segments\n",
    "                    segments = [\"topic\", \"dialogue_history\", \"dialogue_state\"]\n",
    "                    # tokens to separate the segments\n",
    "                    tokens_to_find = [[7082, 441, 28747], [11308, 3829, 28747]]\n",
    "                    for file in item.iterdir():\n",
    "                        if file.name.startswith(\"integrated_gradients\"):\n",
    "                            if file.name.endswith(\"none.pkl\"):\n",
    "                                # name of the segments\n",
    "                                segments = [\"dialogue_history\", \"dialogue_state\"]\n",
    "                                # tokens to separate the segments\n",
    "                                tokens_to_find = [[27304, 441, 1665, 28747]]\n",
    "                            else:\n",
    "                                # name of the segments\n",
    "                                segments = [\"dialogue_history\", \"dialogue_state\", \"knowledge\"]\n",
    "                                # tokens to separate the segments\n",
    "                                tokens_to_find = [[27304, 441, 1665, 28747], [11308, 3829, 28747]]\n",
    "                            with open(file, \"rb\") as f:\n",
    "                                int_grad = pickle.load(f)\n",
    "                            sc, top_k_sc = compute_average_attribution(int_grad, top_k_percentage, tokens_to_remove, tokens_to_find)    \n",
    "                            scores[f'{model.name}_ft_{file.name.split(\"integrated_gradients_\").pop()[:-4]}'] = {k:v for k, v in zip(segments, sc)}\n",
    "                            top_k_scores[f'{model.name}_ft_{file.name.split(\"integrated_gradients_\").pop()[:-4]}'] = {k:v for k, v in zip(segments, top_k_sc)}\n",
    "                elif item.name.startswith(\"integrated_gradients\"):\n",
    "                    # prompt to separate from the rest of the dialogue\n",
    "                    prompt_to_find = [[560, 272, 2296, 7114, 264, 2188, 5659, 298, 6619, 741, 5541, 304, 3208, 1316, 477, 396, 13892, 28723, 13718, 441, 272, 7114, 395, 272, 2899, 302, 272, 13892, 28723]]\n",
    "                    if item.name.endswith(\"none.pkl\"):\n",
    "                        # name of the segments\n",
    "                        segments = [\"prompt\", \"dialogue_history\", \"dialogue_state\"]\n",
    "                        # tokens to separate the segments\n",
    "                        tokens_to_find = [[27304, 441, 1665, 28747]]\n",
    "                    else:\n",
    "                        # name of the segments\n",
    "                        segments = [\"prompt\", \"dialogue_history\", \"dialogue_state\", \"knowledge\"]\n",
    "                        # tokens to separate the segments\n",
    "                        tokens_to_find = [[27304, 441, 1665, 28747], [11308, 3829, 28747]]\n",
    "                    with open(item, \"rb\") as f:\n",
    "                        int_grad = pickle.load(f)\n",
    "                    sc, top_k_sc = compute_average_attribution(int_grad, top_k_percentage, tokens_to_remove, tokens_to_find, prompt_to_find)\n",
    "                    scores[f'{model.name}_prompt_{item.name.split(\"integrated_gradients_\").pop()[:-4]}'] = {k:v for k, v in zip(segments, sc)}\n",
    "                    top_k_scores[f'{model.name}_prompt_{item.name.split(\"integrated_gradients_\").pop()[:-4]}'] = {k:v for k, v in zip(segments, top_k_sc)}\n",
    "        else:\n",
    "            # tokens to remove from the attribution\n",
    "            tokens_to_remove = [1, 2, 518, 25580, 29962, 3532, 14816, 29903, 6778, 29966, 829]\n",
    "            for item in model.iterdir():\n",
    "                if item.is_dir():\n",
    "                    # name of the segments\n",
    "                    segments = [\"topic\", \"dialogue_history\", \"dialogue_state\"]\n",
    "                    for file in item.iterdir():\n",
    "                        if file.name.startswith(\"integrated_gradients\"):\n",
    "                            if file.name.endswith(\"none.pkl\"):\n",
    "                                # name of the segments\n",
    "                                segments = [\"dialogue_history\", \"dialogue_state\"]\n",
    "                                # tokens to separate the segments\n",
    "                                tokens_to_find = [[18878, 434, 2106, 29901]]\n",
    "                            else:\n",
    "                                # name of the segments\n",
    "                                segments = [\"dialogue_history\", \"dialogue_state\", \"knowledge\"]\n",
    "                                # tokens to separate the segments\n",
    "                                tokens_to_find = [[18878, 434, 2106, 29901], [19320, 5485, 29901]]\n",
    "                            with open(file, \"rb\") as f:\n",
    "                                int_grad = pickle.load(f)\n",
    "                            sc, top_k_sc = compute_average_attribution(int_grad, top_k_percentage, tokens_to_remove, tokens_to_find)    \n",
    "                            scores[f'{model.name}_ft_{file.name.split(\"integrated_gradients_\").pop()[:-4]}'] = {k:v for k, v in zip(segments, sc)}\n",
    "                            top_k_scores[f'{model.name}_ft_{file.name.split(\"integrated_gradients_\").pop()[:-4]}'] = {k:v for k, v in zip(segments, top_k_sc)}\n",
    "                elif item.name.startswith(\"integrated_gradients\"):\n",
    "                    # prompt to separate from the rest of the dialogue\n",
    "                    prompt_to_find = [[797, 278, 1494, 14983, 263, 1404, 10753, 304, 6176, 777, 7306, 322, 4225, 1371, 515, 385, 20255, 29889, 2866, 14150, 278, 14983, 411, 278, 2933, 310, 278, 20255, 29889]]\n",
    "                    if item.name.endswith(\"none.pkl\"):\n",
    "                        # name of the segments\n",
    "                        segments = [\"prompt\", \"dialogue_history\", \"dialogue_state\"]\n",
    "                        # tokens to separate the segments\n",
    "                        tokens_to_find = [[18878, 434, 2106, 29901]]\n",
    "                    else:\n",
    "                        # name of the segments\n",
    "                        segments = [\"prompt\", \"dialogue_history\", \"dialogue_state\", \"knowledge\"]\n",
    "                        # tokens to separate the segments\n",
    "                        tokens_to_find = [[18878, 434, 2106, 29901], [19320, 5485, 29901]]\n",
    "                    with open(item, \"rb\") as f:\n",
    "                        int_grad = pickle.load(f)\n",
    "                    sc, top_k_sc = compute_average_attribution(int_grad, top_k_percentage, tokens_to_remove, tokens_to_find, prompt_to_find)\n",
    "                    scores[f'{model.name}_prompt_{item.name.split(\"integrated_gradients_\").pop()[:-4]}'] = {k:v for k, v in zip(segments, sc)}\n",
    "                    top_k_scores[f'{model.name}_prompt_{item.name.split(\"integrated_gradients_\").pop()[:-4]}'] = {k:v for k, v in zip(segments, top_k_sc)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>dialogue_history</th>\n",
       "      <th>dialogue_state</th>\n",
       "      <th>knowledge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>llama_ft_gold</th>\n",
       "      <td>NaN</td>\n",
       "      <td>27.09</td>\n",
       "      <td>13.61</td>\n",
       "      <td>59.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama_prompt_retrieved_top-1</th>\n",
       "      <td>30.28</td>\n",
       "      <td>23.62</td>\n",
       "      <td>12.60</td>\n",
       "      <td>33.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama_ft_retrieved_top-3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>35.59</td>\n",
       "      <td>16.61</td>\n",
       "      <td>47.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama_ft_none</th>\n",
       "      <td>NaN</td>\n",
       "      <td>55.00</td>\n",
       "      <td>45.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama_prompt_none</th>\n",
       "      <td>40.70</td>\n",
       "      <td>31.86</td>\n",
       "      <td>27.44</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama_prompt_gold</th>\n",
       "      <td>28.33</td>\n",
       "      <td>22.37</td>\n",
       "      <td>15.75</td>\n",
       "      <td>33.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama_prompt_retrieved_top-3</th>\n",
       "      <td>31.52</td>\n",
       "      <td>19.47</td>\n",
       "      <td>16.96</td>\n",
       "      <td>32.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama_ft_retrieved_top-1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>27.92</td>\n",
       "      <td>17.22</td>\n",
       "      <td>54.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral_ft_gold</th>\n",
       "      <td>NaN</td>\n",
       "      <td>16.46</td>\n",
       "      <td>31.65</td>\n",
       "      <td>51.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral_prompt_retrieved_top-1</th>\n",
       "      <td>67.28</td>\n",
       "      <td>10.11</td>\n",
       "      <td>15.02</td>\n",
       "      <td>7.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral_ft_retrieved_top-3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>26.74</td>\n",
       "      <td>37.02</td>\n",
       "      <td>36.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral_ft_none</th>\n",
       "      <td>NaN</td>\n",
       "      <td>43.95</td>\n",
       "      <td>56.05</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral_prompt_none</th>\n",
       "      <td>70.67</td>\n",
       "      <td>10.16</td>\n",
       "      <td>19.17</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral_prompt_gold</th>\n",
       "      <td>70.54</td>\n",
       "      <td>10.47</td>\n",
       "      <td>12.01</td>\n",
       "      <td>6.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral_prompt_retrieved_top-3</th>\n",
       "      <td>67.87</td>\n",
       "      <td>13.41</td>\n",
       "      <td>13.97</td>\n",
       "      <td>4.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral_ft_retrieved_top-1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>22.29</td>\n",
       "      <td>28.69</td>\n",
       "      <td>49.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                prompt  dialogue_history  dialogue_state  \\\n",
       "llama_ft_gold                      NaN             27.09           13.61   \n",
       "llama_prompt_retrieved_top-1     30.28             23.62           12.60   \n",
       "llama_ft_retrieved_top-3           NaN             35.59           16.61   \n",
       "llama_ft_none                      NaN             55.00           45.00   \n",
       "llama_prompt_none                40.70             31.86           27.44   \n",
       "llama_prompt_gold                28.33             22.37           15.75   \n",
       "llama_prompt_retrieved_top-3     31.52             19.47           16.96   \n",
       "llama_ft_retrieved_top-1           NaN             27.92           17.22   \n",
       "mistral_ft_gold                    NaN             16.46           31.65   \n",
       "mistral_prompt_retrieved_top-1   67.28             10.11           15.02   \n",
       "mistral_ft_retrieved_top-3         NaN             26.74           37.02   \n",
       "mistral_ft_none                    NaN             43.95           56.05   \n",
       "mistral_prompt_none              70.67             10.16           19.17   \n",
       "mistral_prompt_gold              70.54             10.47           12.01   \n",
       "mistral_prompt_retrieved_top-3   67.87             13.41           13.97   \n",
       "mistral_ft_retrieved_top-1         NaN             22.29           28.69   \n",
       "\n",
       "                                knowledge  \n",
       "llama_ft_gold                       59.31  \n",
       "llama_prompt_retrieved_top-1        33.50  \n",
       "llama_ft_retrieved_top-3            47.80  \n",
       "llama_ft_none                         NaN  \n",
       "llama_prompt_none                     NaN  \n",
       "llama_prompt_gold                   33.55  \n",
       "llama_prompt_retrieved_top-3        32.05  \n",
       "llama_ft_retrieved_top-1            54.86  \n",
       "mistral_ft_gold                     51.89  \n",
       "mistral_prompt_retrieved_top-1       7.58  \n",
       "mistral_ft_retrieved_top-3          36.24  \n",
       "mistral_ft_none                       NaN  \n",
       "mistral_prompt_none                   NaN  \n",
       "mistral_prompt_gold                  6.97  \n",
       "mistral_prompt_retrieved_top-3       4.76  \n",
       "mistral_ft_retrieved_top-1          49.01  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(top_k_scores).T * 100\n",
    "display(df[[\"prompt\", \"dialogue_history\", \"dialogue_state\", \"knowledge\"]].round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knowledge only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import compute_average_attribution\n",
    "\n",
    "with open(\"../data/DSTC9/samples_to_eval.json\", \"r\") as f:\n",
    "    samples_to_eval = json.load(f)\n",
    "\n",
    "# top_k_percentage tokens with highest attribution\n",
    "top_k_percentage = 0.25\n",
    "\n",
    "with_knowledge = True\n",
    "\n",
    "scores = {}\n",
    "top_k_scores = {}\n",
    "for model in Path(\"../output/DSTC9/\").iterdir():\n",
    "    if model.is_dir():\n",
    "        if model.name == \"mistral\":\n",
    "            # tokens to remove from the attribution\n",
    "            tokens_to_remove = [1, 2, 28705, 733, 16289, 28793, 28748]\n",
    "            for item in model.iterdir():\n",
    "                if item.is_dir():\n",
    "                    # name of the segments\n",
    "                    segments = [\"topic\", \"dialogue_history\", \"dialogue_state\"]\n",
    "                    # tokens to separate the segments\n",
    "                    tokens_to_find = [[7082, 441, 28747], [11308, 3829, 28747]]\n",
    "                    for file in item.iterdir():\n",
    "                        if file.name.startswith(\"integrated_gradients\"):\n",
    "                            if file.name.endswith(\"none.pkl\"):\n",
    "                                # name of the segments\n",
    "                                segments = [\"dialogue_history\", \"dialogue_state\"]\n",
    "                                # tokens to separate the segments\n",
    "                                tokens_to_find = [[27304, 441, 1665, 28747]]\n",
    "                            else:\n",
    "                                # name of the segments\n",
    "                                segments = [\"dialogue_history\", \"dialogue_state\", \"knowledge\"]\n",
    "                                # tokens to separate the segments\n",
    "                                tokens_to_find = [[27304, 441, 1665, 28747], [11308, 3829, 28747]]\n",
    "                            with open(file, \"rb\") as f:\n",
    "                                int_grad = pickle.load(f)\n",
    "                                int_grad = filter_by_knowledge(int_grad, samples_to_eval, with_knowledge)\n",
    "                            sc, top_k_sc = compute_average_attribution(int_grad, top_k_percentage, tokens_to_remove, tokens_to_find)    \n",
    "                            scores[f'{model.name}_ft_{file.name.split(\"integrated_gradients_\").pop()[:-4]}'] = {k:v for k, v in zip(segments, sc)}\n",
    "                            top_k_scores[f'{model.name}_ft_{file.name.split(\"integrated_gradients_\").pop()[:-4]}'] = {k:v for k, v in zip(segments, top_k_sc)}\n",
    "                elif item.name.startswith(\"integrated_gradients\"):\n",
    "                    # prompt to separate from the rest of the dialogue\n",
    "                    prompt_to_find = [[560, 272, 2296, 7114, 264, 2188, 5659, 298, 6619, 741, 5541, 304, 3208, 1316, 477, 396, 13892, 28723, 13718, 441, 272, 7114, 395, 272, 2899, 302, 272, 13892, 28723]]\n",
    "                    if item.name.endswith(\"none.pkl\"):\n",
    "                        # name of the segments\n",
    "                        segments = [\"prompt\", \"dialogue_history\", \"dialogue_state\"]\n",
    "                        # tokens to separate the segments\n",
    "                        tokens_to_find = [[27304, 441, 1665, 28747]]\n",
    "                    else:\n",
    "                        # name of the segments\n",
    "                        segments = [\"prompt\", \"dialogue_history\", \"dialogue_state\", \"knowledge\"]\n",
    "                        # tokens to separate the segments\n",
    "                        tokens_to_find = [[27304, 441, 1665, 28747], [11308, 3829, 28747]]\n",
    "                    with open(item, \"rb\") as f:\n",
    "                        int_grad = pickle.load(f)\n",
    "                        int_grad = filter_by_knowledge(int_grad, samples_to_eval, with_knowledge)\n",
    "                    sc, top_k_sc = compute_average_attribution(int_grad, top_k_percentage, tokens_to_remove, tokens_to_find, prompt_to_find)\n",
    "                    scores[f'{model.name}_prompt_{item.name.split(\"integrated_gradients_\").pop()[:-4]}'] = {k:v for k, v in zip(segments, sc)}\n",
    "                    top_k_scores[f'{model.name}_prompt_{item.name.split(\"integrated_gradients_\").pop()[:-4]}'] = {k:v for k, v in zip(segments, top_k_sc)}\n",
    "        else:\n",
    "            # tokens to remove from the attribution\n",
    "            tokens_to_remove = [1, 2, 518, 25580, 29962, 3532, 14816, 29903, 6778, 29966, 829]\n",
    "            for item in model.iterdir():\n",
    "                if item.is_dir():\n",
    "                    # name of the segments\n",
    "                    segments = [\"topic\", \"dialogue_history\", \"dialogue_state\"]\n",
    "                    for file in item.iterdir():\n",
    "                        if file.name.startswith(\"integrated_gradients\"):\n",
    "                            if file.name.endswith(\"none.pkl\"):\n",
    "                                # name of the segments\n",
    "                                segments = [\"dialogue_history\", \"dialogue_state\"]\n",
    "                                # tokens to separate the segments\n",
    "                                tokens_to_find = [[18878, 434, 2106, 29901]]\n",
    "                            else:\n",
    "                                # name of the segments\n",
    "                                segments = [\"dialogue_history\", \"dialogue_state\", \"knowledge\"]\n",
    "                                # tokens to separate the segments\n",
    "                                tokens_to_find = [[18878, 434, 2106, 29901], [19320, 5485, 29901]]\n",
    "                            with open(file, \"rb\") as f:\n",
    "                                int_grad = pickle.load(f)\n",
    "                                int_grad = filter_by_knowledge(int_grad, samples_to_eval, with_knowledge)\n",
    "                            sc, top_k_sc = compute_average_attribution(int_grad, top_k_percentage, tokens_to_remove, tokens_to_find)    \n",
    "                            scores[f'{model.name}_ft_{file.name.split(\"integrated_gradients_\").pop()[:-4]}'] = {k:v for k, v in zip(segments, sc)}\n",
    "                            top_k_scores[f'{model.name}_ft_{file.name.split(\"integrated_gradients_\").pop()[:-4]}'] = {k:v for k, v in zip(segments, top_k_sc)}\n",
    "                elif item.name.startswith(\"integrated_gradients\"):\n",
    "                    # prompt to separate from the rest of the dialogue\n",
    "                    prompt_to_find = [[797, 278, 1494, 14983, 263, 1404, 10753, 304, 6176, 777, 7306, 322, 4225, 1371, 515, 385, 20255, 29889, 2866, 14150, 278, 14983, 411, 278, 2933, 310, 278, 20255, 29889]]\n",
    "                    if item.name.endswith(\"none.pkl\"):\n",
    "                        # name of the segments\n",
    "                        segments = [\"prompt\", \"dialogue_history\", \"dialogue_state\"]\n",
    "                        # tokens to separate the segments\n",
    "                        tokens_to_find = [[18878, 434, 2106, 29901]]\n",
    "                    else:\n",
    "                        # name of the segments\n",
    "                        segments = [\"prompt\", \"dialogue_history\", \"dialogue_state\", \"knowledge\"]\n",
    "                        # tokens to separate the segments\n",
    "                        tokens_to_find = [[18878, 434, 2106, 29901], [19320, 5485, 29901]]\n",
    "                    with open(item, \"rb\") as f:\n",
    "                        int_grad = pickle.load(f)\n",
    "                        int_grad = filter_by_knowledge(int_grad, samples_to_eval, with_knowledge)\n",
    "                    sc, top_k_sc = compute_average_attribution(int_grad, top_k_percentage, tokens_to_remove, tokens_to_find, prompt_to_find)\n",
    "                    scores[f'{model.name}_prompt_{item.name.split(\"integrated_gradients_\").pop()[:-4]}'] = {k:v for k, v in zip(segments, sc)}\n",
    "                    top_k_scores[f'{model.name}_prompt_{item.name.split(\"integrated_gradients_\").pop()[:-4]}'] = {k:v for k, v in zip(segments, top_k_sc)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>dialogue_history</th>\n",
       "      <th>dialogue_state</th>\n",
       "      <th>knowledge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>llama_ft_gold</th>\n",
       "      <td>NaN</td>\n",
       "      <td>27.19</td>\n",
       "      <td>8.03</td>\n",
       "      <td>64.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama_prompt_retrieved_top-1</th>\n",
       "      <td>28.31</td>\n",
       "      <td>21.14</td>\n",
       "      <td>14.16</td>\n",
       "      <td>36.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama_ft_retrieved_top-3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>46.64</td>\n",
       "      <td>12.94</td>\n",
       "      <td>40.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama_ft_none</th>\n",
       "      <td>NaN</td>\n",
       "      <td>65.28</td>\n",
       "      <td>34.72</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama_prompt_none</th>\n",
       "      <td>41.14</td>\n",
       "      <td>31.63</td>\n",
       "      <td>27.24</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama_prompt_gold</th>\n",
       "      <td>25.98</td>\n",
       "      <td>19.54</td>\n",
       "      <td>16.45</td>\n",
       "      <td>38.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama_prompt_retrieved_top-3</th>\n",
       "      <td>38.24</td>\n",
       "      <td>14.81</td>\n",
       "      <td>17.49</td>\n",
       "      <td>29.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama_ft_retrieved_top-1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>36.04</td>\n",
       "      <td>18.72</td>\n",
       "      <td>45.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral_ft_gold</th>\n",
       "      <td>NaN</td>\n",
       "      <td>14.54</td>\n",
       "      <td>29.06</td>\n",
       "      <td>56.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral_prompt_retrieved_top-1</th>\n",
       "      <td>67.34</td>\n",
       "      <td>9.89</td>\n",
       "      <td>13.21</td>\n",
       "      <td>9.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral_ft_retrieved_top-3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>34.05</td>\n",
       "      <td>39.44</td>\n",
       "      <td>26.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral_ft_none</th>\n",
       "      <td>NaN</td>\n",
       "      <td>48.37</td>\n",
       "      <td>51.63</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral_prompt_none</th>\n",
       "      <td>67.61</td>\n",
       "      <td>10.22</td>\n",
       "      <td>22.17</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral_prompt_gold</th>\n",
       "      <td>69.05</td>\n",
       "      <td>10.19</td>\n",
       "      <td>11.24</td>\n",
       "      <td>9.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral_prompt_retrieved_top-3</th>\n",
       "      <td>63.33</td>\n",
       "      <td>14.64</td>\n",
       "      <td>15.42</td>\n",
       "      <td>6.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral_ft_retrieved_top-1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>28.54</td>\n",
       "      <td>30.49</td>\n",
       "      <td>40.97</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                prompt  dialogue_history  dialogue_state  \\\n",
       "llama_ft_gold                      NaN             27.19            8.03   \n",
       "llama_prompt_retrieved_top-1     28.31             21.14           14.16   \n",
       "llama_ft_retrieved_top-3           NaN             46.64           12.94   \n",
       "llama_ft_none                      NaN             65.28           34.72   \n",
       "llama_prompt_none                41.14             31.63           27.24   \n",
       "llama_prompt_gold                25.98             19.54           16.45   \n",
       "llama_prompt_retrieved_top-3     38.24             14.81           17.49   \n",
       "llama_ft_retrieved_top-1           NaN             36.04           18.72   \n",
       "mistral_ft_gold                    NaN             14.54           29.06   \n",
       "mistral_prompt_retrieved_top-1   67.34              9.89           13.21   \n",
       "mistral_ft_retrieved_top-3         NaN             34.05           39.44   \n",
       "mistral_ft_none                    NaN             48.37           51.63   \n",
       "mistral_prompt_none              67.61             10.22           22.17   \n",
       "mistral_prompt_gold              69.05             10.19           11.24   \n",
       "mistral_prompt_retrieved_top-3   63.33             14.64           15.42   \n",
       "mistral_ft_retrieved_top-1         NaN             28.54           30.49   \n",
       "\n",
       "                                knowledge  \n",
       "llama_ft_gold                       64.77  \n",
       "llama_prompt_retrieved_top-1        36.39  \n",
       "llama_ft_retrieved_top-3            40.41  \n",
       "llama_ft_none                         NaN  \n",
       "llama_prompt_none                     NaN  \n",
       "llama_prompt_gold                   38.02  \n",
       "llama_prompt_retrieved_top-3        29.46  \n",
       "llama_ft_retrieved_top-1            45.25  \n",
       "mistral_ft_gold                     56.39  \n",
       "mistral_prompt_retrieved_top-1       9.56  \n",
       "mistral_ft_retrieved_top-3          26.51  \n",
       "mistral_ft_none                       NaN  \n",
       "mistral_prompt_none                   NaN  \n",
       "mistral_prompt_gold                  9.52  \n",
       "mistral_prompt_retrieved_top-3       6.61  \n",
       "mistral_ft_retrieved_top-1          40.97  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(top_k_scores).T * 100\n",
    "display(df[[\"prompt\", \"dialogue_history\", \"dialogue_state\", \"knowledge\"]].round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knowledge only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import compute_average_attribution\n",
    "\n",
    "with open(\"../data/DSTC9/samples_to_eval.json\", \"r\") as f:\n",
    "    samples_to_eval = json.load(f)\n",
    "\n",
    "# top_k_percentage tokens with highest attribution\n",
    "top_k_percentage = 0.25\n",
    "\n",
    "with_knowledge = False\n",
    "\n",
    "scores = {}\n",
    "top_k_scores = {}\n",
    "for model in Path(\"../output/DSTC9/\").iterdir():\n",
    "    if model.is_dir():\n",
    "        if model.name == \"mistral\":\n",
    "            # tokens to remove from the attribution\n",
    "            tokens_to_remove = [1, 2, 28705, 733, 16289, 28793, 28748]\n",
    "            for item in model.iterdir():\n",
    "                if item.is_dir():\n",
    "                    # name of the segments\n",
    "                    segments = [\"topic\", \"dialogue_history\", \"dialogue_state\"]\n",
    "                    # tokens to separate the segments\n",
    "                    tokens_to_find = [[7082, 441, 28747], [11308, 3829, 28747]]\n",
    "                    for file in item.iterdir():\n",
    "                        if file.name.startswith(\"integrated_gradients\"):\n",
    "                            if file.name.endswith(\"none.pkl\"):\n",
    "                                # name of the segments\n",
    "                                segments = [\"dialogue_history\", \"dialogue_state\"]\n",
    "                                # tokens to separate the segments\n",
    "                                tokens_to_find = [[27304, 441, 1665, 28747]]\n",
    "                            else:\n",
    "                                # name of the segments\n",
    "                                segments = [\"dialogue_history\", \"dialogue_state\", \"knowledge\"]\n",
    "                                # tokens to separate the segments\n",
    "                                tokens_to_find = [[27304, 441, 1665, 28747], [11308, 3829, 28747]]\n",
    "                            with open(file, \"rb\") as f:\n",
    "                                int_grad = pickle.load(f)\n",
    "                                int_grad = filter_by_knowledge(int_grad, samples_to_eval, with_knowledge)\n",
    "                            sc, top_k_sc = compute_average_attribution(int_grad, top_k_percentage, tokens_to_remove, tokens_to_find)    \n",
    "                            scores[f'{model.name}_ft_{file.name.split(\"integrated_gradients_\").pop()[:-4]}'] = {k:v for k, v in zip(segments, sc)}\n",
    "                            top_k_scores[f'{model.name}_ft_{file.name.split(\"integrated_gradients_\").pop()[:-4]}'] = {k:v for k, v in zip(segments, top_k_sc)}\n",
    "                elif item.name.startswith(\"integrated_gradients\"):\n",
    "                    # prompt to separate from the rest of the dialogue\n",
    "                    prompt_to_find = [[560, 272, 2296, 7114, 264, 2188, 5659, 298, 6619, 741, 5541, 304, 3208, 1316, 477, 396, 13892, 28723, 13718, 441, 272, 7114, 395, 272, 2899, 302, 272, 13892, 28723]]\n",
    "                    if item.name.endswith(\"none.pkl\"):\n",
    "                        # name of the segments\n",
    "                        segments = [\"prompt\", \"dialogue_history\", \"dialogue_state\"]\n",
    "                        # tokens to separate the segments\n",
    "                        tokens_to_find = [[27304, 441, 1665, 28747]]\n",
    "                    else:\n",
    "                        # name of the segments\n",
    "                        segments = [\"prompt\", \"dialogue_history\", \"dialogue_state\", \"knowledge\"]\n",
    "                        # tokens to separate the segments\n",
    "                        tokens_to_find = [[27304, 441, 1665, 28747], [11308, 3829, 28747]]\n",
    "                    with open(item, \"rb\") as f:\n",
    "                        int_grad = pickle.load(f)\n",
    "                        int_grad = filter_by_knowledge(int_grad, samples_to_eval, with_knowledge)\n",
    "                    sc, top_k_sc = compute_average_attribution(int_grad, top_k_percentage, tokens_to_remove, tokens_to_find, prompt_to_find)\n",
    "                    scores[f'{model.name}_prompt_{item.name.split(\"integrated_gradients_\").pop()[:-4]}'] = {k:v for k, v in zip(segments, sc)}\n",
    "                    top_k_scores[f'{model.name}_prompt_{item.name.split(\"integrated_gradients_\").pop()[:-4]}'] = {k:v for k, v in zip(segments, top_k_sc)}\n",
    "        else:\n",
    "            # tokens to remove from the attribution\n",
    "            tokens_to_remove = [1, 2, 518, 25580, 29962, 3532, 14816, 29903, 6778, 29966, 829]\n",
    "            for item in model.iterdir():\n",
    "                if item.is_dir():\n",
    "                    # name of the segments\n",
    "                    segments = [\"topic\", \"dialogue_history\", \"dialogue_state\"]\n",
    "                    for file in item.iterdir():\n",
    "                        if file.name.startswith(\"integrated_gradients\"):\n",
    "                            if file.name.endswith(\"none.pkl\"):\n",
    "                                # name of the segments\n",
    "                                segments = [\"dialogue_history\", \"dialogue_state\"]\n",
    "                                # tokens to separate the segments\n",
    "                                tokens_to_find = [[18878, 434, 2106, 29901]]\n",
    "                            else:\n",
    "                                # name of the segments\n",
    "                                segments = [\"dialogue_history\", \"dialogue_state\", \"knowledge\"]\n",
    "                                # tokens to separate the segments\n",
    "                                tokens_to_find = [[18878, 434, 2106, 29901], [19320, 5485, 29901]]\n",
    "                            with open(file, \"rb\") as f:\n",
    "                                int_grad = pickle.load(f)\n",
    "                                int_grad = filter_by_knowledge(int_grad, samples_to_eval, with_knowledge)\n",
    "                            sc, top_k_sc = compute_average_attribution(int_grad, top_k_percentage, tokens_to_remove, tokens_to_find)    \n",
    "                            scores[f'{model.name}_ft_{file.name.split(\"integrated_gradients_\").pop()[:-4]}'] = {k:v for k, v in zip(segments, sc)}\n",
    "                            top_k_scores[f'{model.name}_ft_{file.name.split(\"integrated_gradients_\").pop()[:-4]}'] = {k:v for k, v in zip(segments, top_k_sc)}\n",
    "                elif item.name.startswith(\"integrated_gradients\"):\n",
    "                    # prompt to separate from the rest of the dialogue\n",
    "                    prompt_to_find = [[797, 278, 1494, 14983, 263, 1404, 10753, 304, 6176, 777, 7306, 322, 4225, 1371, 515, 385, 20255, 29889, 2866, 14150, 278, 14983, 411, 278, 2933, 310, 278, 20255, 29889]]\n",
    "                    if item.name.endswith(\"none.pkl\"):\n",
    "                        # name of the segments\n",
    "                        segments = [\"prompt\", \"dialogue_history\", \"dialogue_state\"]\n",
    "                        # tokens to separate the segments\n",
    "                        tokens_to_find = [[18878, 434, 2106, 29901]]\n",
    "                    else:\n",
    "                        # name of the segments\n",
    "                        segments = [\"prompt\", \"dialogue_history\", \"dialogue_state\", \"knowledge\"]\n",
    "                        # tokens to separate the segments\n",
    "                        tokens_to_find = [[18878, 434, 2106, 29901], [19320, 5485, 29901]]\n",
    "                    with open(item, \"rb\") as f:\n",
    "                        int_grad = pickle.load(f)\n",
    "                        int_grad = filter_by_knowledge(int_grad, samples_to_eval, with_knowledge)\n",
    "                    sc, top_k_sc = compute_average_attribution(int_grad, top_k_percentage, tokens_to_remove, tokens_to_find, prompt_to_find)\n",
    "                    scores[f'{model.name}_prompt_{item.name.split(\"integrated_gradients_\").pop()[:-4]}'] = {k:v for k, v in zip(segments, sc)}\n",
    "                    top_k_scores[f'{model.name}_prompt_{item.name.split(\"integrated_gradients_\").pop()[:-4]}'] = {k:v for k, v in zip(segments, top_k_sc)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>dialogue_history</th>\n",
       "      <th>dialogue_state</th>\n",
       "      <th>knowledge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>llama_ft_gold</th>\n",
       "      <td>NaN</td>\n",
       "      <td>26.98</td>\n",
       "      <td>19.18</td>\n",
       "      <td>53.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama_prompt_retrieved_top-1</th>\n",
       "      <td>32.26</td>\n",
       "      <td>26.11</td>\n",
       "      <td>11.03</td>\n",
       "      <td>30.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama_ft_retrieved_top-3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>24.54</td>\n",
       "      <td>20.28</td>\n",
       "      <td>55.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama_ft_none</th>\n",
       "      <td>NaN</td>\n",
       "      <td>44.73</td>\n",
       "      <td>55.27</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama_prompt_none</th>\n",
       "      <td>40.27</td>\n",
       "      <td>32.10</td>\n",
       "      <td>27.64</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama_prompt_gold</th>\n",
       "      <td>30.68</td>\n",
       "      <td>25.19</td>\n",
       "      <td>15.05</td>\n",
       "      <td>29.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama_prompt_retrieved_top-3</th>\n",
       "      <td>24.81</td>\n",
       "      <td>24.13</td>\n",
       "      <td>16.42</td>\n",
       "      <td>34.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama_ft_retrieved_top-1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>19.81</td>\n",
       "      <td>15.72</td>\n",
       "      <td>64.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral_ft_gold</th>\n",
       "      <td>NaN</td>\n",
       "      <td>18.37</td>\n",
       "      <td>34.24</td>\n",
       "      <td>47.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral_prompt_retrieved_top-1</th>\n",
       "      <td>67.23</td>\n",
       "      <td>10.33</td>\n",
       "      <td>16.84</td>\n",
       "      <td>5.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral_ft_retrieved_top-3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>19.43</td>\n",
       "      <td>34.61</td>\n",
       "      <td>45.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral_ft_none</th>\n",
       "      <td>NaN</td>\n",
       "      <td>39.53</td>\n",
       "      <td>60.47</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral_prompt_none</th>\n",
       "      <td>73.74</td>\n",
       "      <td>10.09</td>\n",
       "      <td>16.17</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral_prompt_gold</th>\n",
       "      <td>72.04</td>\n",
       "      <td>10.76</td>\n",
       "      <td>12.79</td>\n",
       "      <td>4.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral_prompt_retrieved_top-3</th>\n",
       "      <td>72.42</td>\n",
       "      <td>12.17</td>\n",
       "      <td>12.51</td>\n",
       "      <td>2.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral_ft_retrieved_top-1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>16.04</td>\n",
       "      <td>26.89</td>\n",
       "      <td>57.06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                prompt  dialogue_history  dialogue_state  \\\n",
       "llama_ft_gold                      NaN             26.98           19.18   \n",
       "llama_prompt_retrieved_top-1     32.26             26.11           11.03   \n",
       "llama_ft_retrieved_top-3           NaN             24.54           20.28   \n",
       "llama_ft_none                      NaN             44.73           55.27   \n",
       "llama_prompt_none                40.27             32.10           27.64   \n",
       "llama_prompt_gold                30.68             25.19           15.05   \n",
       "llama_prompt_retrieved_top-3     24.81             24.13           16.42   \n",
       "llama_ft_retrieved_top-1           NaN             19.81           15.72   \n",
       "mistral_ft_gold                    NaN             18.37           34.24   \n",
       "mistral_prompt_retrieved_top-1   67.23             10.33           16.84   \n",
       "mistral_ft_retrieved_top-3         NaN             19.43           34.61   \n",
       "mistral_ft_none                    NaN             39.53           60.47   \n",
       "mistral_prompt_none              73.74             10.09           16.17   \n",
       "mistral_prompt_gold              72.04             10.76           12.79   \n",
       "mistral_prompt_retrieved_top-3   72.42             12.17           12.51   \n",
       "mistral_ft_retrieved_top-1         NaN             16.04           26.89   \n",
       "\n",
       "                                knowledge  \n",
       "llama_ft_gold                       53.84  \n",
       "llama_prompt_retrieved_top-1        30.61  \n",
       "llama_ft_retrieved_top-3            55.18  \n",
       "llama_ft_none                         NaN  \n",
       "llama_prompt_none                     NaN  \n",
       "llama_prompt_gold                   29.08  \n",
       "llama_prompt_retrieved_top-3        34.63  \n",
       "llama_ft_retrieved_top-1            64.47  \n",
       "mistral_ft_gold                     47.38  \n",
       "mistral_prompt_retrieved_top-1       5.61  \n",
       "mistral_ft_retrieved_top-3          45.96  \n",
       "mistral_ft_none                       NaN  \n",
       "mistral_prompt_none                   NaN  \n",
       "mistral_prompt_gold                  4.41  \n",
       "mistral_prompt_retrieved_top-3       2.90  \n",
       "mistral_ft_retrieved_top-1          57.06  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(top_k_scores).T * 100\n",
    "display(df[[\"prompt\", \"dialogue_history\", \"dialogue_state\", \"knowledge\"]].round(2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inlg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
